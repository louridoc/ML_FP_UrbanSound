{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from scipy.io import wavfile as wav\n",
    "import os\n",
    "from datetime import datetime \n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    nmfcc=40\n",
    "    hop_length = 512\n",
    "    n_fft = 1024\n",
    "    n_mels = 128\n",
    "    try:\n",
    "        y, sr = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        audio, _ = librosa.effects.trim(y)\n",
    "        S = librosa.feature.mfcc(audio, sr=sr, n_fft=n_fft, hop_length=hop_length, center=False, n_mfcc=nmfcc, fmin=0)\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "        mfccsscaled = np.mean(S_DB.T,axis=0)   \n",
    "    except Exception:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    return mfccsscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                feature  class_label\n",
      "0     [-80.0, -2.5677357, -80.0, -80.0, -80.0, -80.0...            3\n",
      "1     [-80.0, -1.000009, -80.0, -3.5282154, -51.5522...            2\n",
      "2     [-80.0, -0.9102828, -80.0, -4.7280188, -51.951...            2\n",
      "3     [-80.0, -1.4973112, -77.357445, -4.4096923, -4...            2\n",
      "4     [-80.0, -0.58883995, -80.0, -3.3096602, -45.63...            2\n",
      "...                                                 ...          ...\n",
      "8727  [-80.0, -0.64906764, -80.0, -6.3651896, -75.83...            1\n",
      "8728  [-80.0, -2.813218, -80.0, -4.689935, -80.0, -5...            1\n",
      "8729  [-80.0, -1.5988646, -80.0, -6.2588015, -74.091...            1\n",
      "8730  [-80.0, -0.66870856, -80.0, -6.298651, -80.0, ...            1\n",
      "8731  [-80.0, -1.0094088, -80.0, -4.0532303, -72.861...            1\n",
      "\n",
      "[8732 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Label filepath\n",
    "fulldatasetpath = 'Audio_set/'\n",
    "metadata = pd.read_csv('metadata/UrbanSound8k.csv')\n",
    "\n",
    "## Check for audio in labels and get features\n",
    "features = []\n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),str(row[\"slice_file_name\"]))\n",
    "    class_label = row[\"classID\"]\n",
    "    data = extract_features(file_name)\n",
    "    features.append([data, class_label])\n",
    "    \n",
    "## Convert to pandas dataframe\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=6):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs) \n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs) \n",
    "        \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  \n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  \n",
    "        ## (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  \n",
    "        ## (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    ## For calling multihead attention on embedded data and arranging it sequentially and adding other layers.\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    ## For preliminary token generation and embedding\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                feature  class_label\n",
      "0     [-80.0, -2.5677357, -80.0, -80.0, -80.0, -80.0...            3\n",
      "1     [-80.0, -1.000009, -80.0, -3.5282154, -51.5522...            2\n",
      "2     [-80.0, -0.9102828, -80.0, -4.7280188, -51.951...            2\n",
      "3     [-80.0, -1.4973112, -77.357445, -4.4096923, -4...            2\n",
      "4     [-80.0, -0.58883995, -80.0, -3.3096602, -45.63...            2\n",
      "...                                                 ...          ...\n",
      "8727  [-80.0, -0.64906764, -80.0, -6.3651896, -75.83...            1\n",
      "8728  [-80.0, -2.813218, -80.0, -4.689935, -80.0, -5...            1\n",
      "8729  [-80.0, -1.5988646, -80.0, -6.2588015, -74.091...            1\n",
      "8730  [-80.0, -0.66870856, -80.0, -6.298651, -80.0, ...            1\n",
      "8731  [-80.0, -1.0094088, -80.0, -4.0532303, -72.861...            1\n",
      "\n",
      "[8732 rows x 2 columns]\n",
      "8000000\n",
      "[[      0 7743226       0 ... 2573972 2629853 3770590]\n",
      " [      0 7899999       0 ... 3708962 2854796 3255666]\n",
      " [      0 7908971       0 ... 3848537 3874896 2767926]\n",
      " ...\n",
      " [      0 7840113       0 ... 2173984 4407086 5151892]\n",
      " [      0 7933129       0 ...  333429 2475616 5325988]\n",
      " [      0 7899059       0 ... 3489432   56613  933560]] (8732, 40)\n",
      "[3 2 2 ... 1 1 1] (8732,)\n"
     ]
    }
   ],
   "source": [
    "## Generating audio data in the word domain to apply transformer\n",
    "featuresdf=featuresdf.dropna(axis=0)\n",
    "print(featuresdf)\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "X=X*100000\n",
    "min_X=-min([min(element) for element in X])\n",
    "x=X+min_X\n",
    "x=x.astype(int)\n",
    "max_len=max([max(element) for element in x])\n",
    "print(max_len)\n",
    "# Getting label size\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "print(x,x.shape)\n",
    "print(y,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732 40\n",
      "(8732,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1,shuffle=False)\n",
    "m,n=x.shape\n",
    "print(m,n)\n",
    "print(y.shape)\n",
    "x_train=x[0:int(m*9/10),:]\n",
    "x_test=x[int(m*9/10):m,:]\n",
    "y_train=y[0:int(m*9/10)]\n",
    "y_test=y[int(m*9/10):m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874 40\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'int'>\n",
      "<class 'list'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n",
      "7858 40\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'int'>\n",
      "<class 'list'>\n",
      "<class 'numpy.int32'>\n",
      "<class 'numpy.ndarray'>\n",
      "(7858,) (7858,) (874,) (874,)\n"
     ]
    }
   ],
   "source": [
    "## Changing data type for transformer layers\n",
    "m,n=x_test.shape\n",
    "print(m,n)\n",
    "\n",
    "## Converting form and reshaping\n",
    "TestX=x_test\n",
    "TestY=y_test\n",
    "testy=np.reshape(TestY,(m,))\n",
    "\n",
    "## Changing datatypes\n",
    "testx=np.empty((m,),object)\n",
    "for i in range (0,m):\n",
    "    testx[i]=list(int(v) for v in TestX[i])\n",
    "    testy[i]=testy[i].astype(int)\n",
    "\n",
    "## Printing data-types - relevant to transformer input\n",
    "print(type(testx))\n",
    "print(type(testx[m-1][n-1]))\n",
    "print(type(testx[m-1]))\n",
    "print(type(testy[m-1]))\n",
    "print(type(testy))\n",
    "\n",
    "## Converting Train Data and Getting size of data\n",
    "m,n=x_train.shape\n",
    "print(m,n)\n",
    "\n",
    "## Converting form and reshaping\n",
    "TrainX=x_train\n",
    "TrainY=y_train\n",
    "trainy=np.reshape(TrainY,(m,))\n",
    "\n",
    "## Changing datatypes\n",
    "trainx=np.empty((m,),object)\n",
    "for i in range (0,m):\n",
    "    trainx[i]=list(int(v) for v in TrainX[i])\n",
    "    trainy[i]=TrainY[i].astype(int)\n",
    "    \n",
    "## Printing data-types - relevant to transformer input\n",
    "print(type(trainx))\n",
    "print(type(trainx[m-1][n-1]))\n",
    "print(type(trainx[m-1]))\n",
    "print(type(trainy[m-1]))\n",
    "print(type(trainy))\n",
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7858 Training sequences\n",
      "874 Validation sequences\n",
      "(7858, 40) (7858,) (874, 40) (874,)\n",
      "[[      0 7743226       0 ... 2573972 2629853 3770590]\n",
      " [      0 7899999       0 ... 3708962 2854796 3255666]\n",
      " [      0 7908971       0 ... 3848537 3874896 2767926]\n",
      " ...\n",
      " [      0 7853487       0 ... 4667272 2068970 3188562]\n",
      " [      0 7759655       0 ... 5486369 3050592 3924683]\n",
      " [      0 7792581       0 ... 5073622 3182251 5185436]] [3 2 2 ... 4 4 4] [[      0 7886826  110820 ... 2201447 5018277 4557853]\n",
      " [      0 7911832       0 ... 2387120 3787225 3220354]\n",
      " [      0 7902066   32901 ... 3111667 4550398 2754965]\n",
      " ...\n",
      " [      0 7840113       0 ... 2173984 4407086 5151892]\n",
      " [      0 7933129       0 ...  333429 2475616 5325988]\n",
      " [      0 7899059       0 ... 3489432   56613  933560]] [3 3 3 4 4 4 4 4 4 4 8 8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 3 3 3 3 3 6 6 1 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 4 4 4 4 4 4\n",
      " 9 9 9 9 9 9 9 3 3 3 3 9 9 9 9 9 9 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 9 9 9 9 9 9 3 3 3 3 3 3 3 3 3 0 0 0 0 0 2 2\n",
      " 2 2 2 2 2 2 5 5 5 5 5 5 5 5 4 4 4 4 4 4 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 2 2 3 3 3 3 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 2 2 2 2 2 2 2 2\n",
      " 9 9 9 9 9 9 6 3 2 2 2 2 2 2 2 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
      " 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 6 3 3 3 1 1 1 1 1 1 6 8 8 8 8 8 8 7\n",
      " 2 2 2 2 2 2 2 4 4 4 3 3 3 3 3 3 9 9 9 9 9 9 9 9 9 9 9 9 1 1 5 5 5 5 5 5 5\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 3 3 3 3 3 3 3 8 5 5 5 5 5 5 5 5\n",
      " 5 8 8 8 8 5 5 5 5 2 2 2 2 2 2 2 9 9 9 9 9 9 6 9 9 9 9 9 9 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 1 9 9 9 9 9 9 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 5 5 5 5 5 1 1 1 5 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 8 8 8 8 1 1 1 9 9 9 9 9 9 9 9 9 9 9 9 3 3 3 3 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 3 7 7 7 7 7 7 3 9 9 9 9 9 9 7 7 7 7 7 7 7 7 7 7\n",
      " 7 9 9 9 9 9 9 8 8 9 9 9 9 9 9 9 9 9 9 9 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 2 2 2 2 2 2 2 2 9 9 9 9 9 9 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max_len+1\n",
    "maxlen = 40\n",
    "print(len(trainx), \"Training sequences\")\n",
    "print(len(testx), \"Validation sequences\")\n",
    "\n",
    "## Converting to padded tensor sequence\n",
    "trainx = keras.preprocessing.sequence.pad_sequences(trainx,maxlen=maxlen)\n",
    "testx = keras.preprocessing.sequence.pad_sequences(testx,maxlen=maxlen)\n",
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)\n",
    "print(trainx,trainy,testx,testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 40, 30)            240001230 \n",
      "_________________________________________________________________\n",
      "transformer_block_1 (Transfo (None, 40, 30)            5700      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 40, 6)             546       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 40, 30)            210       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 40, 30)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 30)            0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20, 30)            930       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 20, 30)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 240,011,716\n",
      "Trainable params: 240,011,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 30  ## Embedding size for each token\n",
    "num_heads = 6  ## Number of attention heads\n",
    "ff_dim = 30  ## Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "## Tokenizing input data with max dimension and embedding it\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "#x = keras.Sequential()\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "## Adding Sequential layer to the embedded data and attention layers too.\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "\n",
    "## Add other layers\n",
    "x = layers.Conv1D(6,3,padding=\"same\")(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(30, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "## Producing general softmax layer for classification\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "## Generating model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79/79 [==============================] - 162s 2s/step - loss: 2.3054 - accuracy: 0.1089\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 163s 2s/step - loss: 2.2808 - accuracy: 0.1121\n",
      "Epoch 3/5\n",
      " 7/79 [=>............................] - ETA: 2:17 - loss: 2.2819 - accuracy: 0.1057"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-437676b053b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(trainx, trainy, batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance [0.48297321796417236, 0.8677664995193481]\n",
      "Testing Performanr [0.6155197620391846, 0.8627991080284119]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(trainx, trainy, verbose=0)\n",
    "print(\"Training Performance\",score)\n",
    "score = model.evaluate(testx, testy, verbose=0)\n",
    "print(\"Testing Performanr\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
